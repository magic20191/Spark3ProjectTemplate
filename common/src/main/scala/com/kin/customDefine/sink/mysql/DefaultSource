package com.kin.customDefine.sink.mysql
import com.kin.utils.DruidUtils

import java.sql.DriverManager
import java.util
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.connector.catalog._
import org.apache.spark.sql.connector.expressions.Transform
import org.apache.spark.sql.connector.write._
import org.apache.spark.sql.types.{LongType, StringType, StructField, StructType}
import org.apache.spark.sql.util.CaseInsensitiveStringMap

import scala.collection.JavaConverters._

/*
  * Default source should some kind of relation provider
  * reference by: http://blog.madhukaraphatak.com/spark-3-datasource-v2-part-6/
  */

class DefaultSource extends TableProvider{
  override def inferSchema(caseInsensitiveStringMap: CaseInsensitiveStringMap): StructType =
    getTable(null,Array.empty[Transform],caseInsensitiveStringMap.asCaseSensitiveMap()).schema()

  override def getTable(structType: StructType, transforms: Array[Transform], map: util.Map[String, String]): Table ={
    new MysqlTable(structType)
  }
}

class MysqlTable(structType: StructType) extends SupportsWrite{

  override def name(): String = this.getClass.toString

  /**
   * 设置schema
   */
  override def schema(): StructType = {
    //外部传递，则使用外部参数，没有传递则使用schema
    if(structType != null){
      structType
    }else{
      new StructType()
        .add("id", StringType)
        .add("name", StringType)
        .add("gender", StringType)
        .add("salary", StringType)
        .add("comm", StringType)
    }
  }

  override def capabilities(): util.Set[TableCapability] = Set(TableCapability.BATCH_WRITE,
    TableCapability.TRUNCATE).asJava

  override def newWriteBuilder(logicalWriteInfo: LogicalWriteInfo): WriteBuilder = new MysqlWriterBuilder
}

class MysqlWriterBuilder extends WriteBuilder{
  override def buildForBatch(): BatchWrite = new MysqlBatchWriter()
}

class MysqlBatchWriter extends BatchWrite{
  override def createBatchWriterFactory(physicalWriteInfo: PhysicalWriteInfo): DataWriterFactory = new
      MysqlDataWriterFactory

  override def commit(writerCommitMessages: Array[WriterCommitMessage]): Unit = {}

  override def abort(writerCommitMessages: Array[WriterCommitMessage]): Unit = {}
}

class MysqlDataWriterFactory extends DataWriterFactory {
  override def createWriter(partitionId: Int, taskId:Long): DataWriter[InternalRow] = new MysqlWriter()
}

object WriteSucceeded extends WriterCommitMessage

class MysqlWriter extends DataWriter[InternalRow] {
  val url = "jdbc:mysql://127.0.0.1:3306/test"
  val user = "root"
  val password = "root"
  val table ="userwrite"

//  val connection = DruidUtils.getConnection(url,user,password)
  val connection = DruidUtils.getConnection
  val statement = "insert into userwrite (user) values (?)"
  val preparedStatement = connection.prepareStatement(statement)


  override def write(record: InternalRow): Unit = {
    println(record.getString(0) + "  " +record.getString(2))
    val value = record.getString(1)
    preparedStatement.setString(1,value)
    preparedStatement.executeUpdate()
  }

  override def commit(): WriterCommitMessage = WriteSucceeded

  override def abort(): Unit = {
    println("commit is failure !")
  }

  override def close(): Unit = {

  }

}








