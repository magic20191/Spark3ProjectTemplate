package com.kin.test.spark_hudi


import com.kin.common.server.InitialImpl
import org.apache.hudi.DataSourceWriteOptions
import org.apache.hudi.QuickstartUtils.getQuickstartWriteConfigs
import org.apache.hudi.common.model.HoodieAvroPayload
import org.apache.hudi.config.HoodieWriteConfig.TBL_NAME
import org.apache.spark.sql.SaveMode.Overwrite
import test.HoodieExampleDataGenerator

import java.util
import scala.collection.JavaConversions._

/**
 * @ClassName SparkHudiTest
 * @Deacription TODO
 * @Author zy
 * @Date 2022/2/17 14:49
 * @Version 1.0
 * */
class SparkHudiTest extends InitialImpl {

  def testMain(parameterMap: util.Map[String, String]): Unit = {
    println("hello world")
  }

  def testReadHudi(parameterMap: util.Map[String, String]): Unit = {
    //     val tablePath: String = "/hudi/data"
    val tablePath: String = "/hudi/test2/"
    spark.sparkContext.setLogLevel("ERROR")
    val resDf = spark.
      read.
      format("org.apache.hudi").
      load(tablePath + "/*/*/*/*")
    resDf.createOrReplaceTempView("hudi_ro_table")

    spark.sql("select fare, begin_lon, begin_lat, ts from  hudi_ro_table where fare > 20.0").show()
    //  +-----------------+-------------------+-------------------+---+
    //  |             fare|          begin_lon|          begin_lat| ts|
    //  +-----------------+-------------------+-------------------+---+
    //  |98.88075495133515|0.39556048623031603|0.17851135255091155|0.0|
    //  ...

    spark.sql("select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_ro_table").show()
    resDf.show(100, true)
  }

  def testWirteHudi(parameterMap: util.Map[String, String]): Unit = {
    spark.sparkContext.setLogLevel("ERROR")
    val dataGen = new HoodieExampleDataGenerator[HoodieAvroPayload]
    val commitTime: String = System.currentTimeMillis().toString
    val inserts: util.List[String] = dataGen.convertToStringList(dataGen.generateInserts(commitTime, 20))
    println(inserts)
    val df = spark.read.json(spark.sparkContext.parallelize(inserts, 1))
    df.write.format("org.apache.hudi").
      options(getQuickstartWriteConfigs).
      option(DataSourceWriteOptions.PRECOMBINE_FIELD.key, "ts").
      option(DataSourceWriteOptions.RECORDKEY_FIELD.key, "uuid").
      option(DataSourceWriteOptions.PARTITIONPATH_FIELD.key, "partitionpath").
      option(TBL_NAME.key, "hudi_test1").
      mode(Overwrite).
      save("/hudi/test3/")
  }
}
